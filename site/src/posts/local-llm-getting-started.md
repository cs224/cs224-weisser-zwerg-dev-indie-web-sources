---
layout: "layouts/post-with-toc.njk"
title: "Running AI Models on Your PC: A Guide to Local Large Language Models (LLMs)"
description: "Setting Up AI Models on Older Hardware - A Beginnerâ€™s Guide to Running Local LLMs with Limited Resources"
creationdate: 2024-10-26
keywords: local llm, ollama, gpt4all, vllm, qwen 2.5, llama model, ministral, AI on PC, local large language models
date: 2024-10-26
tags: ['post']
---

## Rationale

Recently, I read a blog post titled [Alibaba Releases Qwen 2.5 Models, Raising the Bar for Open-Weight LLMs](https://www.deeplearning.ai/the-batch/alibaba-releases-qwen-2-5-models-raising-the-bar-for-open-weight-llms/).
This release sparked my interest in testing out these open-weight LLMs myself.
Therefore, I decided to see how they would run on my older 2017 PC, which has a [GeForce GTX 1080](https://www.nvidia.com/en-ph/geforce/products/10series/geforce-gtx-1080) graphics card with 8GB of VRAM.
If you're curious about running AI models on your own hardware, this post walks through my findings and shows what you can achieve even with older equipment.
